{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e03773",
   "metadata": {},
   "source": [
    "## LAB AGENTES - PARTE 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ad734",
   "metadata": {},
   "source": [
    "## Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb3eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \\\n",
    "    python-dotenv \\\n",
    "    langchain \\\n",
    "    langchain-chroma \\\n",
    "    langchain-google-genai \\\n",
    "    langchain-text-splitters \\\n",
    "    langchain-core \\\n",
    "    langgraph \\\n",
    "    langgraph-checkpoint-sqlite \\\n",
    "    ddgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91d2b3",
   "metadata": {},
   "source": [
    "## Aplicaci√≥n RAG basada en LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe24257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar variables desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Asignar la clave de API a la variable de entorno que espera el SDK de Google\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ab61e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "def load_and_chunk_markdown_documents(data_folder = \"data\"):\n",
    "\n",
    "        \"\"\"\n",
    "    Carga todos los documentos Markdown desde la carpeta especificada,\n",
    "    aplicando segmentaci√≥n basada en estructura de encabezados para\n",
    "    generar chunks sem√°nticamente coherentes, listos para el proceso\n",
    "    de embedding.\n",
    "\n",
    "    Args:\n",
    "        data_folder (str): Carpeta que contiene los archivos .md\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de objetos Document (chunks) enriquecidos con metadatos\n",
    "    \"\"\"\n",
    "\n",
    "        print(\"\\nCARGA Y CHUNKING DE DOCUMENTOS EN FORMATO MARKDOWN\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        # Definir niveles jer√°rquicos que guiar√°n la segmentaci√≥n\n",
    "        headers_to_split_on = [\n",
    "                (\"#\", \"Header 1\"),\n",
    "                (\"##\", \"Header 2\"),\n",
    "                (\"###\", \"Header 3\"),\n",
    "                (\"####\", \"Header 4\"),\n",
    "        ]\n",
    "\n",
    "        # Segmentador basado en estructura de documento\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "                headers_to_split_on = headers_to_split_on,\n",
    "                strip_headers = False\n",
    "        )\n",
    "\n",
    "        # Verificar existencia de la carpeta y archivos Markdown\n",
    "        data_path = Path(data_folder)\n",
    "        if not data_path.exists():\n",
    "                raise FileNotFoundError(f\"\\nLa carpeta '{data_folder}' no existe\")\n",
    "\n",
    "        # Verificar que existan archivos a segmentar\n",
    "        markdown_files = list(data_path.glob(\"*.md\"))\n",
    "        if not markdown_files:\n",
    "                raise FileNotFoundError(f\"\\nNo se encontraron archivos .md en '{data_folder}'\")\n",
    "\n",
    "        all_chunks = []\n",
    "\n",
    "        print(f\"\\nProcesando {len(markdown_files)} archivos:\\n\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        for file_path in sorted(markdown_files):\n",
    "                try:\n",
    "                        # Leer contenido crudo del archivo\n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                                content = file.read()\n",
    "\n",
    "                        # Aplicar segmentaci√≥n estructural\n",
    "                        chunks = markdown_splitter.split_text(content)\n",
    "\n",
    "                        # Enriquecer cada chunk con metadatos √∫tiles\n",
    "                        for i, chunk in enumerate(chunks):\n",
    "                                chunk.metadata.update({\n",
    "                                        \"source_file\": file_path.name,\n",
    "                                        \"source_path\": str(file_path),\n",
    "                                        \"chunk_index\": i,\n",
    "                                        \"total_chunks_in_doc\": len(chunks),\n",
    "                                        \"chunking_strategy\": \"document_structured\",\n",
    "                                        \"file_size\": file_path.stat().st_size,\n",
    "                                        \"chunk_size\": len(chunk.page_content)\n",
    "                                })\n",
    "\n",
    "                        all_chunks.extend(chunks)\n",
    "                        print(f\"{file_path.name}: {len(chunks)} chunks generados\")\n",
    "\n",
    "                except Exception as e:\n",
    "                        print(f\"\\nError procesando {file_path.name}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Total chunks generados: {len(all_chunks)}\")\n",
    "\n",
    "        # Mostrar estad√≠sticas generales\n",
    "        if all_chunks:\n",
    "                chunk_sizes = [len(chunk.page_content) for chunk in all_chunks]\n",
    "                print(f\"Tama√±o promedio de chunks: {sum(chunk_sizes) / len(chunk_sizes):.0f} caracteres\")\n",
    "                print(f\"Rango de tama√±os: {min(chunk_sizes)} - {max(chunk_sizes)} caracteres\")\n",
    "                print(\"-\" * 30)\n",
    "\n",
    "        return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a7ef194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_documents_for_rag(data_folder = \"data\"):\n",
    "    \n",
    "        \"\"\"\n",
    "    Funci√≥n completa que prepara documentos para el pipeline RAG.\n",
    "\n",
    "    Returns:\n",
    "        list: Chunks listos para crear embeddings y guardar en base de datos vectorial\n",
    "    \"\"\"\n",
    "\n",
    "        # Cargar y hacer chunking de todos los documentos\n",
    "        chunks = load_and_chunk_markdown_documents(data_folder)\n",
    "\n",
    "\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d07a8c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import shutil\n",
    "\n",
    "def load_to_chromadb(chunks, collection_name = \"rag_docs\", persist_directory = \"./chroma_db\"):\n",
    "    \n",
    "        \"\"\"\n",
    "    Funci√≥n original mantenida para cargar documentos a ChromaDB.\n",
    "    Esta funci√≥n se usa antes de crear el grafo de LangGraph.\n",
    "    \"\"\"\n",
    "       \n",
    "        # Eliminar la base anterior si existe para asegurar una carga limpia\n",
    "        if os.path.exists(persist_directory):\n",
    "                shutil.rmtree(persist_directory)\n",
    "\n",
    "        # Instanciar el modelo de embeddings de Google\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
    "\n",
    "        # Crear una base de datos vectorial persistente con la colecci√≥n especificada\n",
    "        vector_store = Chroma(\n",
    "                collection_name = collection_name,\n",
    "                embedding_function = embeddings,\n",
    "                persist_directory = persist_directory\n",
    "        )\n",
    "\n",
    "        # Extraer los textos de cada chunk\n",
    "        texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "        # Extraer los metadatos asociados\n",
    "        metadatas = [chunk.metadata for chunk in chunks]\n",
    "\n",
    "        # Generar identificadores √∫nicos para cada chunk\n",
    "        ids = [f\"{chunk.metadata.get('source_file', 'doc')}_{i}\" for i, chunk in enumerate(chunks)]\n",
    "\n",
    "        # Agregar los embeddings y metadatos a la colecci√≥n\n",
    "        vector_store.add_texts(texts = texts, metadatas = metadatas, ids = ids)\n",
    "\n",
    "        print(f\"\\nChromaDB: {len(chunks)} chunks cargados\")\n",
    "        return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53af667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CARGA Y CHUNKING DE DOCUMENTOS EN FORMATO MARKDOWN\n",
      "======================================================================\n",
      "\n",
      "Procesando 5 archivos:\n",
      "\n",
      "------------------------------\n",
      "api_development_standards.md: 20 chunks generados\n",
      "development_policies.md: 28 chunks generados\n",
      "multi_theme_descriptions.md: 7 chunks generados\n",
      "software_architecture_guide.md: 14 chunks generados\n",
      "troubleshooting_guide.md: 8 chunks generados\n",
      "------------------------------\n",
      "Total chunks generados: 77\n",
      "Tama√±o promedio de chunks: 413 caracteres\n",
      "Rango de tama√±os: 94 - 1692 caracteres\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763086931.991374   23594 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ChromaDB: 77 chunks cargados\n"
     ]
    }
   ],
   "source": [
    "# Generaci√≥n de los chunks finales\n",
    "final_chunks = prepare_documents_for_rag()\n",
    "# Carga de los chunks a la base de datos vectorial\n",
    "vector_store = load_to_chromadb(final_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f041d255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1763086934.164508   23594 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Instancia del modelo a utilizar\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    temperature = 0.1,\n",
    "    max_tokens = 200\n",
    ")\n",
    "\n",
    "# Promp base del asistente\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Eres un asistente especializado en responder preguntas sobre documentaci√≥n t√©cnica de desarrollo de software. Utiliza √∫nicamente la informaci√≥n del contexto proporcionado para\n",
    "        responder la pregunta. Si no conoces la respuesta bas√°ndote en el contexto, indica claramente que no tienes esa informaci√≥n. Mant√©n las respuestas concisas, precisas y usa m√°ximo\n",
    "        tres oraciones.\n",
    "\n",
    "        Pregunta: {question}\n",
    "\n",
    "        Contexto: {context}\n",
    "\n",
    "        Respuesta:\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c9941",
   "metadata": {},
   "source": [
    "## Asistente conversacional tipo ChatBot con LangGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4536ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Any, Dict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class RAGState(TypedDict):\n",
    "    \n",
    "        \"\"\"\n",
    "    Estado centralizado para el sistema RAG con LangGraph.\n",
    "    Reemplaza el manejo impl√≠cito de datos del c√≥digo original.\n",
    "    \"\"\"\n",
    "        # Entrada del usuario\n",
    "        question: str\n",
    "\n",
    "        # Par√°metros de recuperaci√≥n\n",
    "        k: int\n",
    "        search_type: str\n",
    "\n",
    "        # Datos de documentos recuperados\n",
    "        retrieved_docs: List[Document]\n",
    "        context_text: str\n",
    "\n",
    "        # Respuesta generada\n",
    "        answer: str\n",
    "        sources: List[str]\n",
    "\n",
    "        # Metadatos adicionales\n",
    "        vector_store: Any  # Almacenamos referencia al vector store\n",
    "        processing_metadata: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cff87b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def retrieve_documents(state: RAGState) -> RAGState:\n",
    "    \n",
    "        \"\"\"\n",
    "    Nodo de recuperaci√≥n de documentos.\n",
    "    Migraci√≥n directa de la l√≥gica del ask_rag original.\n",
    "    \"\"\"\n",
    "      \n",
    "        print(\"üîç Ejecutando nodo: retrieve_documents\")\n",
    "\n",
    "        # Obtenci√≥n de par√°metros almacenados en el estado compartido\n",
    "        question = state[\"question\"]\n",
    "        vector_store = state[\"vector_store\"]\n",
    "        k = state.get(\"k\", 3)\n",
    "\n",
    "        # Recuperar documentos relevantes (igual que el original)\n",
    "        retrieved_docs = vector_store.similarity_search(question, k=k)\n",
    "\n",
    "        # Registrar metadatos del proceso\n",
    "        processing_metadata = state.get(\"processing_metadata\", {})\n",
    "        processing_metadata.update({\n",
    "                \"documents_retrieved\": len(retrieved_docs),\n",
    "                \"retrieval_timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        # Actualizar estado con documentos recuperados\n",
    "        return {\n",
    "                **state,\n",
    "                \"retrieved_docs\": retrieved_docs,\n",
    "                \"processing_metadata\": processing_metadata\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03905bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context(state: RAGState) -> RAGState:\n",
    "    \n",
    "        \"\"\"\n",
    "    Nodo de preparaci√≥n de contexto.\n",
    "    Separa la l√≥gica de preparaci√≥n del contexto en un nodo independiente.\n",
    "    \"\"\"\n",
    "      \n",
    "        print(\"üìù Ejecutando nodo: prepare_context\")\n",
    "\n",
    "        # Obtenci√≥n de par√°mteros almacenados en el estado compartido\n",
    "        retrieved_docs = state[\"retrieved_docs\"]\n",
    "\n",
    "        # Preparar el contexto concatenando contenido \n",
    "        context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "        # Extraer fuentes\n",
    "        sources = [doc.metadata.get('source_file', 'unknown') for doc in retrieved_docs]\n",
    "\n",
    "        # Actualizar metadatos\n",
    "        processing_metadata = state.get(\"processing_metadata\", {})\n",
    "        processing_metadata.update({\n",
    "                \"context_length\": len(context_text),\n",
    "                \"sources_count\": len(set(sources)),\n",
    "                \"context_preparation_timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "        return {\n",
    "                **state,\n",
    "                \"context_text\": context_text,\n",
    "                \"sources\": sources,\n",
    "                \"processing_metadata\": processing_metadata\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77aadd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(state: RAGState) -> RAGState:\n",
    "    \n",
    "        \"\"\"\n",
    "    Nodo de generaci√≥n de respuesta.\n",
    "    \"\"\"\n",
    "      \n",
    "        print(\"ü§ñ Ejecutando nodo: generate_response\")\n",
    "\n",
    "        # Obtenci√≥n de par√°metros almacenados en el estado compartido\n",
    "        question = state[\"question\"]\n",
    "        context_text = state[\"context_text\"]\n",
    "\n",
    "        # Construir mensaje usando el prompt original\n",
    "        messages = prompt.invoke({\n",
    "                \"question\": question,\n",
    "                \"context\": context_text\n",
    "        })\n",
    "\n",
    "        # Paso 4: Generar respuesta con el LLM (igual que el original)\n",
    "        response = llm.invoke(messages)\n",
    "\n",
    "        # Actualizar metadatos finales\n",
    "        processing_metadata = state.get(\"processing_metadata\", {})\n",
    "        processing_metadata.update({\n",
    "                \"response_length\": len(response.content),\n",
    "                \"generation_timestamp\": datetime.now().isoformat(),\n",
    "                \"model_used\": \"gemini-2.5-flash\"\n",
    "        })\n",
    "\n",
    "        return {\n",
    "                **state,\n",
    "                \"answer\": response.content,\n",
    "                \"processing_metadata\": processing_metadata\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf5871",
   "metadata": {},
   "source": [
    "## Ensamblaje, verificaci√≥n y visualizaci√≥n del grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a8ccf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "def create_rag_graph():\n",
    "    \n",
    "        \"\"\"\n",
    "    Crea el grafo RAG usando LangGraph StateGraph.\n",
    "        Se unen los nodos utilizando ejes, para su ejecuci√≥n secuencial.\n",
    "    \"\"\"\n",
    "      \n",
    "        # Inicializar el grafo con nuestro estado tipado\n",
    "        workflow = StateGraph(RAGState)\n",
    "\n",
    "        # Agregar nodos de procesamiento\n",
    "        workflow.add_node(\"retrieve\", retrieve_documents)\n",
    "        workflow.add_node(\"prepare_context\", prepare_context) \n",
    "        workflow.add_node(\"generate\", generate_response)\n",
    "\n",
    "        # A√±ade los ejes y define el punto de inicio y de fin del flujo\n",
    "        workflow.add_edge(START, \"retrieve\")\n",
    "        workflow.add_edge(\"retrieve\", \"prepare_context\")\n",
    "        workflow.add_edge(\"prepare_context\", \"generate\")\n",
    "        workflow.add_edge(\"generate\", END)\n",
    "\n",
    "        return workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34784142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "def show_graph():\n",
    "    \n",
    "        \"\"\"\n",
    "    Muestra el grafo en una celda interactiva. \n",
    "        Esta configurada espec√≠ficamente para notebook jupyter.\n",
    "    \"\"\"\n",
    "    \n",
    "        app = create_rag_graph().compile()\n",
    "    \n",
    "        try:\n",
    "                display(Image(app.get_graph().draw_mermaid_png()))\n",
    "        except ImportError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba8eed",
   "metadata": {},
   "source": [
    "## Ejecuci√≥n del asistente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f3a8811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag_langgraph(question: str, vector_store, k: int = 3, search_type: str = \"similarity\"):\n",
    "    \n",
    "        \"\"\"\n",
    "    Interfaz equivalente al ask_rag original usando LangGraph.\n",
    "    Mantiene la misma signatura para compatibilidad hacia atr√°s.\n",
    "    \n",
    "    Args:\n",
    "        question (str): Pregunta del usuario.\n",
    "        vector_store: Base vectorial configurada previamente.\n",
    "        k (int): N√∫mero de fragmentos a recuperar.\n",
    "        search_type (str): Tipo de b√∫squeda (mantenido para compatibilidad).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Resultado con el mismo formato que ask_rag original.\n",
    "    \"\"\"\n",
    "    \n",
    "        # Crear y compilar el grafo\n",
    "        workflow = create_rag_graph()\n",
    "        app = workflow.compile()\n",
    "        # Muestra el grafo para su verificaci√≥n\n",
    "        show_graph() \n",
    "\n",
    "        # Estado inicial, listo para ser usado por el flujo\n",
    "        initial_state = {\n",
    "                \"question\": question,\n",
    "                \"k\": k,\n",
    "                \"search_type\": search_type,\n",
    "                \"vector_store\": vector_store,\n",
    "                \"retrieved_docs\": [],\n",
    "                \"context_text\": \"\",\n",
    "                \"answer\": \"\",\n",
    "                \"sources\": [],\n",
    "                \"processing_metadata\": {\n",
    "                        \"start_timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "        }\n",
    "\n",
    "        print(\"üöÄ Iniciando procesamiento RAG con LangGraph\")\n",
    "        final_state = app.invoke(initial_state)\n",
    "\n",
    "        # Retornar en formato compatible con ask_rag original\n",
    "        result = {\n",
    "                \"question\": final_state[\"question\"],\n",
    "                \"context\": final_state[\"retrieved_docs\"],  \n",
    "                \"answer\": final_state[\"answer\"],\n",
    "                \"sources\": final_state[\"sources\"]\n",
    "        }\n",
    "\n",
    "        print(\"‚úÖ Procesamiento completado\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6fcb36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Iniciando procesamiento RAG con LangGraph\n",
      "üîç Ejecutando nodo: retrieve_documents\n",
      "üìù Ejecutando nodo: prepare_context\n",
      "ü§ñ Ejecutando nodo: generate_response\n",
      "‚úÖ Procesamiento completado\n",
      "\n",
      "‚úÖ RESULTADO FINAL:\n",
      "ü§ñ Respuesta: Los m√©todos HTTP son GET, POST, PUT, PATCH y DELETE. Cada uno tiene un prop√≥sito espec√≠fico, como obtener, crear, actualizar o eliminar recursos.\n",
      "üìÑ Fuentes utilizadas: api_development_standards.md, api_development_standards.md, api_development_standards.md\n",
      "üìä Documentos procesados: 3\n"
     ]
    }
   ],
   "source": [
    "# Pregunta de prueba\n",
    "question = \"Cu√°les son los m√©todos HTTP?\"\n",
    "\n",
    "# Ejecuci√≥n del flujo crado \n",
    "result = ask_rag_langgraph(\n",
    "    question = question,\n",
    "    vector_store = vector_store,\n",
    "    k = 3,\n",
    "    search_type = \"similarity\"\n",
    ")\n",
    "\n",
    "# Visualizaci√≥n de los resultados\n",
    "print(\"\\n‚úÖ RESULTADO FINAL:\")\n",
    "print(f\"ü§ñ Respuesta: {result['answer']}\")\n",
    "print(f\"üìÑ Fuentes utilizadas: {', '.join(result['sources'])}\")\n",
    "print(f\"üìä Documentos procesados: {len(result['context'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854c4b7",
   "metadata": {},
   "source": [
    "# LAB AGENTES - PARTE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec36cd",
   "metadata": {},
   "source": [
    "## A√±adiendo memoria conversacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba8620f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "\n",
    "class MemoryRAGState(TypedDict):\n",
    "    \n",
    "        \"\"\"\n",
    "    Estado RAG expandido con memoria conversacional\n",
    "    \n",
    "    A√±ade capacidades de memoria al estado original:\n",
    "    - conversation_history: Historial autom√°tico de mensajes\n",
    "    - session_id: Identificador de conversaci√≥n\n",
    "    - user_context: Informaci√≥n personal extra√≠da autom√°ticamente\n",
    "    \n",
    "    NOTA: El vector_store no se incluye para evitar problemas de serializaci√≥n\n",
    "    \"\"\"\n",
    "      \n",
    "    # Campos originales del RAG\n",
    "        question: str\n",
    "        k: int\n",
    "        retrieved_docs: List[Document]\n",
    "        context_text: str\n",
    "        answer: str\n",
    "        sources: List[str]\n",
    "        processing_metadata: Dict[str, Any]\n",
    "\n",
    "        # Campos nuevos para manejo de memoria conversacional\n",
    "        conversation_history: Annotated[List[Dict[str, Any]], add_messages]\n",
    "        session_id: str\n",
    "        user_context: Dict[str, Any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81c4ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "import sqlite3\n",
    "\n",
    "def create_memory_checkpointer(db_path: str = \"langgraph_memory.db\"):\n",
    "    \n",
    "        \"\"\"\n",
    "    Crea checkpointer SQLite para memoria persistente autom√°tica\n",
    "    \n",
    "    LangGraph manejar√° autom√°ticamente:\n",
    "    - Guardado de estado despu√©s de cada nodo\n",
    "    - Recuperaci√≥n de estado por session_id\n",
    "    - Persistencia entre sesiones\n",
    "    \"\"\"\n",
    "    \n",
    "        # Creaci√≥n de la conexi√≥n con la base de datos\n",
    "        conn = sqlite3.connect(db_path, check_same_thread = False)\n",
    "        return SqliteSaver(conn)\n",
    "\n",
    "# Crear checkpointer global\n",
    "memory_checkpointer = create_memory_checkpointer()\n",
    "\n",
    "# Variable global para vector_store (no serializable)\n",
    "global_vector_store = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52d2d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_memory_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Eres un asistente especializado en responder preguntas sobre documentaci√≥n t√©cnica de desarrollo de software. \n",
    "\n",
    "        IMPORTANTE: Mant√©n la continuidad conversacional usando el historial de conversaci√≥n proporcionado.\n",
    "\n",
    "        {conversation_history}\n",
    "\n",
    "        Utiliza √∫nicamente la informaci√≥n del contexto proporcionado para responder la pregunta actual. \n",
    "        Si no conoces la respuesta bas√°ndote en el contexto, indica claramente que no tienes esa informaci√≥n.\n",
    "\n",
    "        Si el usuario se ha presentado o ha proporcionado informaci√≥n personal en conversaciones anteriores, √∫sala para personalizar tu respuesta cuando sea apropiado.\n",
    "\n",
    "        Mant√©n las respuestas concisas, precisas y usa m√°ximo tres oraciones.\n",
    "\n",
    "        Pregunta actual: {question}\n",
    "\n",
    "        Contexto de documentaci√≥n: {context}\n",
    "\n",
    "        Respuesta:\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b717955b",
   "metadata": {},
   "source": [
    "## Uso de la memoria para personalizar recuperaci√≥n y generaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6624c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_user_context_comprehensive(history):\n",
    "    \n",
    "        \"\"\"\n",
    "    Extrae informaci√≥n personal de usuario a partir del historial\n",
    "        de conversaci√≥n. √ötil para personalizar las respuestas del modelo. \n",
    "    \"\"\"\n",
    "    \n",
    "        # Estructura par manejar la inforamci√≥n contextual del usuario\n",
    "        user_context = {\n",
    "                # Lista de todos los mensajes con informaci√≥n personal \n",
    "                \"personal_info_messages\": [], \n",
    "                # Mensaje m√°s reciente con informaci√≥n personal \n",
    "                \"latest_personal_info\": \"\",    \n",
    "                # Informaci√≥n personal consolidada\n",
    "                \"consolidated_info\": \"\"        \n",
    "        }\n",
    "\n",
    "        # Lista de palabras que suelen denotar el inicio de informaci√≥n asociada con el usuario\n",
    "        # Se puede ampliar\n",
    "        personal_keywords = [\"soy\", \"me llamo\", \"trabajo\",\n",
    "                                                \"especializo\", \"estudi√≥\", \"vivo en\", \"tengo experiencia\", \n",
    "                                                \"mi nombre\", \"trabajo como\", \"me dedico\"]\n",
    "\n",
    "        # Revisi√≥n de los mensajes en el historial \n",
    "        for msg in history:\n",
    "                content = \"\"\n",
    "                if hasattr(msg, 'type') and msg.type == \"human\":\n",
    "                        content = msg.content\n",
    "                elif hasattr(msg, 'get') and msg.get(\"role\") == \"user\":  \n",
    "                        content = msg.get(\"content\", \"\")\n",
    "                \n",
    "                # Revisi√≥n de informaci√≥n personal en el mesaje\n",
    "                if content and any(word in content.lower() for word in personal_keywords):\n",
    "                        user_context[\"personal_info_messages\"].append(content)\n",
    "                        user_context[\"latest_personal_info\"] = content  \n",
    "\n",
    "        # Consolidaci√≥n de toda la informaci√≥n personal \n",
    "        if user_context[\"personal_info_messages\"]:\n",
    "                user_context[\"consolidated_info\"] = \" | \".join(user_context[\"personal_info_messages\"])\n",
    "\n",
    "        return user_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6ac21e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_retrieve_documents(state: MemoryRAGState) -> MemoryRAGState:\n",
    "    \n",
    "        \"\"\"\n",
    "    Nodo de recuperaci√≥n que usa contexto conversacional.\n",
    "    Maneja la informaci√≥n personal del usuario y el proceso cl√°sico de\n",
    "    recuperaci√≥n de informaci√≥n. \n",
    "    \"\"\"\n",
    "\n",
    "        # Obtenci√≥n de informaci√≥n almacenada en el contexto\n",
    "        question = state[\"question\"]\n",
    "        vector_store = global_vector_store\n",
    "        history = state.get(\"conversation_history\", [])\n",
    "\n",
    "        # Extracci√≥n de informaci√≥n personal del usuario a partir del historial de mensajes\n",
    "        user_context = extract_user_context_comprehensive(history)\n",
    "\n",
    "        # Si hay historial, se utiliza para mejorar el contexto\n",
    "        if history:\n",
    "                recent_questions = []\n",
    "                # Revisa los √∫ltimos dos mensajes del usuario\n",
    "                for msg in history[-2:]: \n",
    "                        if hasattr(msg, 'type') and msg.type == \"human\":\n",
    "                                recent_questions.append(msg.content)\n",
    "                        elif hasattr(msg, 'get') and msg.get(\"role\") == \"user\":  \n",
    "                                recent_questions.append(msg.get(\"content\", \"\"))\n",
    "                \n",
    "                # Se agregan los ultimos dos mensaje al contexto, ademas de la pregunta\n",
    "                enhanced_query = f\"{question} [Contexto: {' | '.join(recent_questions)}]\"\n",
    "        else:\n",
    "                enhanced_query = question\n",
    "\n",
    "        # Recuperaci√≥n de documentos\n",
    "        retrieved_docs = vector_store.similarity_search(enhanced_query, k = state.get(\"k\", 3))\n",
    "\n",
    "        new_user_message = {\"role\": \"user\", \"content\": question}\n",
    "        updated_history = history + [new_user_message]\n",
    "\n",
    "        return {\n",
    "                **state,\n",
    "                \"retrieved_docs\": retrieved_docs,\n",
    "                \"user_context\": user_context,\n",
    "                \"conversation_history\": updated_history,\n",
    "                \"processing_metadata\": {\n",
    "                        **state.get(\"processing_metadata\", {}),\n",
    "                        \"used_conversation_context\": len(history) > 0,\n",
    "                        \"enhanced_query\": enhanced_query,\n",
    "                        \"personal_info_count\": len(user_context[\"personal_info_messages\"])\n",
    "                }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d82c835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_prepare_context(state: MemoryRAGState) -> MemoryRAGState:\n",
    "    \n",
    "        \"\"\"\n",
    "    Nodo que prepara contexto y extrae informaci√≥n del usuario del historial.\n",
    "    Se encarga de preparar todo el contexto necesario para la generaci√≥n \n",
    "    de respuestas. \n",
    "    \"\"\"\n",
    "      \n",
    "        print(\"üìù Ejecutando nodo: prepare_context\")\n",
    "\n",
    "        # Obtenci√≥n de informaci√≥n almacenada en el estado compartido\n",
    "        retrieved_docs = state[\"retrieved_docs\"]\n",
    "        history = state.get(\"conversation_history\", [])\n",
    "\n",
    "        # Prepara el contexto concatenando el contenido \n",
    "        context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "        # Extrae las fuentes asociadas con el contexto\n",
    "        sources = [doc.metadata.get('source_file', 'unknown') for doc in retrieved_docs]\n",
    "\n",
    "        # Obtiene el contexto personal del usuario\n",
    "        user_context = extract_user_context_comprehensive(history)\n",
    "\n",
    "        return {\n",
    "                **state,\n",
    "                \"context_text\": context_text,\n",
    "                \"sources\": sources,\n",
    "                \"user_context\": user_context,\n",
    "                \"processing_metadata\": {\n",
    "                        **state.get(\"processing_metadata\", {}),\n",
    "                        \"context_length\": len(context_text),\n",
    "                        \"sources_count\": len(set(sources)),\n",
    "                        \"total_personal_messages\": len(user_context[\"personal_info_messages\"])\n",
    "                }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f09ffd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_generate_response(state: MemoryRAGState) -> MemoryRAGState:\n",
    "    \n",
    "        \"\"\"\n",
    "    Nodo de generaci√≥n personalizada con memoria.\n",
    "    Se incorpora toda la incorporaci√≥n recuperada en el prompt para que \n",
    "        el modelo la entienda y la tenga en cuenta. \n",
    "    \"\"\"\n",
    "    \n",
    "        # Obtenci√≥n de la informaci√≥n almacenada en el estado compartido \n",
    "        question = state[\"question\"]\n",
    "        context_text = state[\"context_text\"]\n",
    "        history = state.get(\"conversation_history\", [])\n",
    "        user_context = state.get(\"user_context\", {})\n",
    "\n",
    "        # Formateo del historial de la conversaci√≥n manejando objetos nativos de LangChain\n",
    "        conversation_history = \"\"\n",
    "        if history:\n",
    "                conversation_history = \"HISTORIAL DE CONVERSACI√ìN:\\n\"\n",
    "                for i, msg in enumerate(history, 1):\n",
    "                        if hasattr(msg, 'type'):  \n",
    "                                role = \"Usuario\" if msg.type == \"human\" else \"Asistente\"\n",
    "                                content = msg.content\n",
    "                        else:  \n",
    "                                role = \"Usuario\" if msg.get(\"role\") == \"user\" else \"Asistente\"\n",
    "                                content = msg.get(\"content\", \"\")\n",
    "                        # Se a√±ade el contenido recuperado al historial \n",
    "                        conversation_history += f\"{i}. {role}: {content}\\n\"\n",
    "\n",
    "                conversation_history += \"\\n\"\n",
    "                \n",
    "                # A√±ade la informaci√≥n personal del usuario si se encuentra disponible\n",
    "                if user_context.get(\"consolidated_info\"):\n",
    "                        conversation_history += f\"INFORMACI√ìN PERSONAL DEL USUARIO: {user_context['consolidated_info']}\\n\\n\"\n",
    "        \n",
    "        else:\n",
    "                conversation_history = \"Esta es la primera pregunta de la conversaci√≥n.\\n\\n\"\n",
    "\n",
    "        # Usa el prompt definido anteriormente para recopilar toda la informaci√≥n\n",
    "        messages = conversational_memory_prompt.invoke({\n",
    "                \"conversation_history\": conversation_history,\n",
    "                \"question\": question,\n",
    "                \"context\": context_text\n",
    "        })\n",
    "\n",
    "        # Generaci√≥n de la respuesta\n",
    "        response = llm.invoke(messages)\n",
    "        new_assistant_message = {\"role\": \"assistant\", \"content\": response.content}\n",
    "        updated_history = history + [new_assistant_message]\n",
    "\n",
    "        return {\n",
    "                **state,\n",
    "                \"answer\": response.content,\n",
    "                \"conversation_history\": updated_history,\n",
    "                \"processing_metadata\": {\n",
    "                        **state.get(\"processing_metadata\", {}),\n",
    "                        \"used_formatted_history\": len(history) > 0,\n",
    "                        \"used_personal_context\": bool(user_context.get(\"consolidated_info\"))\n",
    "                }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb62eaa",
   "metadata": {},
   "source": [
    "## Construcci√≥n del grafo con memoria conversacional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18ea3084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memory_rag_graph():\n",
    "    \"\"\"\n",
    "    Crea grafo LangGraph con memoria conversacional autom√°tica\n",
    "    \n",
    "    Ventajas sobre implementaci√≥n manual:\n",
    "    - LangGraph maneja autom√°ticamente el estado entre nodos\n",
    "    - Checkpointer persiste todo autom√°ticamente\n",
    "    - Recuperaci√≥n de estado por session_id\n",
    "    - Rollback y debugging built-in\n",
    "    \"\"\"\n",
    "    \n",
    "    workflow = StateGraph(MemoryRAGState)\n",
    "    \n",
    "    # A√±adir nodos con memoria\n",
    "    workflow.add_node(\"retrieve\", memory_retrieve_documents)\n",
    "    workflow.add_node(\"prepare_context\", memory_prepare_context)\n",
    "    workflow.add_node(\"generate\", memory_generate_response)\n",
    "    \n",
    "    # Flujo sin cambios\n",
    "    workflow.add_edge(START, \"retrieve\")\n",
    "    workflow.add_edge(\"retrieve\", \"prepare_context\")\n",
    "    workflow.add_edge(\"prepare_context\", \"generate\")\n",
    "    workflow.add_edge(\"generate\", END)\n",
    "    \n",
    "    # Compilar con checkpointer para manejo autom√°tico de sesi√≥n \n",
    "    return workflow.compile(checkpointer = memory_checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa996cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_app = create_memory_rag_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e157af1",
   "metadata": {},
   "source": [
    "## Ejecuci√≥n, pruebas e inspecci√≥n del asistente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cb892c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag_with_langgraph_memory(question: str, vector_store, session_id: str = None, k: int = 3):\n",
    "    \n",
    "        \"\"\"\n",
    "    RAG con memoria usando LangGraph\n",
    "    \n",
    "    ¬øQu√© hace LangGraph autom√°ticamente?\n",
    "    - Persiste el estado despu√©s de cada nodo\n",
    "    - Recupera historial por session_id\n",
    "    - Maneja la memoria conversacional\n",
    "    - Permite debugging paso a paso\n",
    "    \"\"\"\n",
    "    \n",
    "        # Establece el vector store como variable global para evitar errores de serializaci√≥n\n",
    "        global global_vector_store\n",
    "        global_vector_store = vector_store  \n",
    "\n",
    "        # Crea un id de sesi√≥n si uno no es pasado como par√°metro\n",
    "        if not session_id:\n",
    "                import uuid\n",
    "                session_id = str(uuid.uuid4())[:8]\n",
    "\n",
    "        # Estado inicial (SIN vector_store para evitar error de serializaci√≥n)\n",
    "        initial_state = {\n",
    "                \"question\": question,\n",
    "                \"k\": k,\n",
    "                \"retrieved_docs\": [],\n",
    "                \"context_text\": \"\",\n",
    "                \"answer\": \"\",\n",
    "                \"sources\": [],\n",
    "                \"conversation_history\": [],  \n",
    "                \"session_id\": session_id,\n",
    "                \"user_context\": {},\n",
    "                \"processing_metadata\": {}\n",
    "        }\n",
    "\n",
    "        # Configuraci√≥n para checkpointer\n",
    "        config = {\"configurable\": {\"thread_id\": session_id}}\n",
    "\n",
    "        final_state = memory_app.invoke(initial_state, config = config)\n",
    "\n",
    "        return {\n",
    "                \"question\": final_state[\"question\"],\n",
    "                \"answer\": final_state[\"answer\"],\n",
    "                \"sources\": [doc.metadata.get('source_file', 'unknown') for doc in final_state[\"retrieved_docs\"]],\n",
    "                \"session_id\": session_id,\n",
    "                \"used_memory\": final_state[\"processing_metadata\"].get(\"used_conversation_context\", False),\n",
    "                \"formatted_history\": final_state[\"processing_metadata\"].get(\"used_formatted_history\", False)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbb9f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def new_conversation():\n",
    "    \n",
    "        \"\"\"\n",
    "    Crea una nueva sesi√≥n de conversaci√≥n.\n",
    "    Establece el id para la sesi√≥n\n",
    "    \"\"\"\n",
    "    \n",
    "        return str(uuid.uuid4())[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0972ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_chat(question, vector_store, session_id):\n",
    "    \n",
    "        \"\"\"\n",
    "    Continua una conversaci√≥n existente haciendo uso del id de la sesi√≥n. \n",
    "    \"\"\"\n",
    "    \n",
    "        result = ask_rag_with_langgraph_memory(question, vector_store, session_id)\n",
    "        print(f\"üë§ Usuario: {question}\")\n",
    "        print(f\"ü§ñ {result['answer']}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b961dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_test():\n",
    "    \n",
    "        \"\"\"\n",
    "    Test simulado para verificar el correcto funcionamiento del flujo.\n",
    "    \"\"\"\n",
    "      \n",
    "        session = new_conversation()\n",
    "        print(f\"üÜï Nueva sesi√≥n creada: {session}\")\n",
    "        print(\"Test 1:\")\n",
    "        continue_chat(\"Soy Ana, desarrolladora React\", vector_store, session)\n",
    "        print(\"\\nTest 2:\")\n",
    "        continue_chat(\"¬øQu√© frameworks React recomiendas para alguien como yo?\", vector_store, session)\n",
    "        print(\"\\nTest 3:\")\n",
    "        continue_chat(\"¬øQu√© son las APIs REST?\", vector_store, session)\n",
    "        print(\"\\nTest 4:\")\n",
    "        continue_chat(\"¬øQu√© pasa cuando tengo Alta latencia en respuestas de API?\", vector_store, session)\n",
    "        print(\"\\nTest 5:\")\n",
    "        continue_chat(\"¬øRecuerdas mi nombre?\", vector_store, session)\n",
    "        return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44eb9acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï Nueva sesi√≥n creada: a5a32924\n",
      "Test 1:\n",
      "üìù Ejecutando nodo: prepare_context\n",
      "üë§ Usuario: Soy Ana, desarrolladora React\n",
      "ü§ñ \n",
      "\n",
      "Test 2:\n",
      "üìù Ejecutando nodo: prepare_context\n",
      "üë§ Usuario: ¬øQu√© frameworks React recomiendas para alguien como yo?\n",
      "ü§ñ \n",
      "\n",
      "Test 3:\n",
      "üìù Ejecutando nodo: prepare_context\n",
      "üë§ Usuario: ¬øQu√© son las APIs REST?\n",
      "ü§ñ \n",
      "\n",
      "Test 4:\n",
      "üìù Ejecutando nodo: prepare_context\n",
      "üë§ Usuario: ¬øQu√© pasa cuando tengo Alta latencia en respuestas de API?\n",
      "ü§ñ Hola Ana, cuando tienes alta latencia en respuestas de API, los s√≠ntomas incluyen tiempos de respuesta superiores a 2 segundos y timeouts frecuentes. Esto puede llevar a quejas de los usuarios sobre la lentitud de la aplicaci√≥n.\n",
      "\n",
      "Test 5:\n",
      "üìù Ejecutando nodo: prepare_context\n",
      "üë§ Usuario: ¬øRecuerdas mi nombre?\n",
      "ü§ñ S√≠, Ana, recuerdo que te llamas Ana.\n"
     ]
    }
   ],
   "source": [
    "example_id = quick_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a60c0660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def inspect_langgraph_state(session_id: str):\n",
    "\n",
    "        \"\"\"\n",
    "    Inspecciona el estado guardado por LangGraph\n",
    "    √ötil para conocer la configuraci√≥n guardada en el estado compartido y ver el proceso \n",
    "    paso a paso.\n",
    "    \"\"\"\n",
    "    \n",
    "        config = {\"configurable\": {\"thread_id\": session_id}}\n",
    "\n",
    "        try:\n",
    "                # Obtener estado actual del checkpointer\n",
    "                state_snapshot = memory_app.get_state(config)\n",
    "                \n",
    "                if state_snapshot.values:\n",
    "                        history = state_snapshot.values.get(\"conversation_history\", [])\n",
    "                        user_context = state_snapshot.values.get(\"user_context\", {})\n",
    "                        \n",
    "                        # Informac√≥n b√°sica presente en el estado\n",
    "                        print(f\"\\nüîç ESTADO DE SESI√ìN: {session_id}\")\n",
    "                        print(\"=\" * 50)\n",
    "                        print(f\"üí¨ Mensajes en historial: {len(history)}\")\n",
    "                        print(f\"üë§ Contexto de usuario: {user_context.get('consolidated_info', 'Sin informaci√≥n personal')}\")\n",
    "                        print(f\"üè∑Ô∏è √öltimo checkpoint: {state_snapshot.config}\")\n",
    "                        \n",
    "                        # Extracci√≥n del historial de mensajes, en caso de que exista\n",
    "                        if history:\n",
    "                                print(f\"\\nüìñ √öltimos {min(3, len(history))} mensajes:\")\n",
    "                                print(\"-\" * 40)\n",
    "                                \n",
    "                                for i, msg in enumerate(history[-3:], len(history)-2 if len(history) > 3 else 1):\n",
    "                                        if hasattr(msg, 'type'):  # It's a LangChain message object\n",
    "                                                role = \"üë§ Usuario\" if msg.type == \"human\" else \"ü§ñ Asistente\"\n",
    "                                                content = msg.content\n",
    "                                        else:  \n",
    "                                                role = \"üë§ Usuario\" if msg.get(\"role\") == \"user\" else \"ü§ñ Asistente\"\n",
    "                                                content = msg.get(\"content\", \"\")\n",
    "                                        \n",
    "                                        display_content = content[:80] + \"...\" if len(content) > 80 else content\n",
    "                                        print(f\"{i:2d}. {role}: {display_content}\")\n",
    "                                \n",
    "                                # Estad√≠sticas adicionales\n",
    "                                print(f\"\\nüìä ESTAD√çSTICAS:\")\n",
    "                                print(f\"   üó®Ô∏è  Total conversaciones: {len(history)}\")\n",
    "                                \n",
    "                                # Conteo de mensajes seg√∫n rol\n",
    "                                user_msgs = 0\n",
    "                                assistant_msgs = 0\n",
    "                                for msg in history:\n",
    "                                        if hasattr(msg, 'type'):\n",
    "                                                if msg.type == \"human\":\n",
    "                                                        user_msgs += 1\n",
    "                                                else:\n",
    "                                                        assistant_msgs += 1\n",
    "                                        else:\n",
    "                                                if msg.get(\"role\") == \"user\":\n",
    "                                                        user_msgs += 1\n",
    "                                                else:\n",
    "                                                        assistant_msgs += 1\n",
    "                                \n",
    "                                print(f\"   üë§ Mensajes de usuario: {user_msgs}\")\n",
    "                                print(f\"   ü§ñ Respuestas del asistente: {assistant_msgs}\")\n",
    "                                \n",
    "                                # Visualizaci√≥n de informaci√≥n personal\n",
    "                                if user_context.get('personal_info_messages'):\n",
    "                                        print(f\"   üìã Info personal recopilada: {len(user_context['personal_info_messages'])} mensaje(s)\")\n",
    "                                        print(f\"   üìù √öltima info personal: {user_context.get('latest_personal_info', 'N/A')[:50]}...\")\n",
    "                                \n",
    "                        else:\n",
    "                                print(\"\\nüì≠ No hay mensajes en el historial\")\n",
    "                                \n",
    "                else:\n",
    "                        print(f\"‚ö†Ô∏è No se encontr√≥ estado para sesi√≥n: {session_id}\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "                print(f\"‚ùå Error inspeccionando estado: {e}\")\n",
    "                print(f\"üîß Detalles del error:\\n{traceback.format_exc()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93d41511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç ESTADO DE SESI√ìN: a5a32924\n",
      "==================================================\n",
      "üí¨ Mensajes en historial: 10\n",
      "üë§ Contexto de usuario: Soy Ana, desarrolladora React | ¬øRecuerdas mi nombre?\n",
      "üè∑Ô∏è √öltimo checkpoint: {'configurable': {'thread_id': 'a5a32924', 'checkpoint_ns': '', 'checkpoint_id': '1f0c1013-61c6-6b01-8017-96a20bd766e9'}}\n",
      "\n",
      "üìñ √öltimos 3 mensajes:\n",
      "----------------------------------------\n",
      " 8. ü§ñ Asistente: Hola Ana, cuando tienes alta latencia en respuestas de API, los s√≠ntomas incluye...\n",
      " 9. üë§ Usuario: ¬øRecuerdas mi nombre?\n",
      "10. ü§ñ Asistente: S√≠, Ana, recuerdo que te llamas Ana.\n",
      "\n",
      "üìä ESTAD√çSTICAS:\n",
      "   üó®Ô∏è  Total conversaciones: 10\n",
      "   üë§ Mensajes de usuario: 5\n",
      "   ü§ñ Respuestas del asistente: 5\n",
      "   üìã Info personal recopilada: 2 mensaje(s)\n",
      "   üìù √öltima info personal: ¬øRecuerdas mi nombre?...\n"
     ]
    }
   ],
   "source": [
    "inspect_langgraph_state(session_id = example_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a4a0e",
   "metadata": {},
   "source": [
    "# LAB AGENTES - PARTE 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c2b89",
   "metadata": {},
   "source": [
    "## A√±adiendo b√∫squedas en internet para respuestas actualizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbbf4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear checkpointer global\n",
    "memory_checkpointer = create_memory_checkpointer(db_path = \"langgraph_memory_with_search.db\")\n",
    "\n",
    "# Variable global para vector_store (no serializable)\n",
    "global_vector_store = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef91554",
   "metadata": {},
   "source": [
    "## Actualizaci√≥n del prompt y nuevos nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0872bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_search_web(state: MemoryRAGState) -> dict:\n",
    "    \n",
    "        \"\"\"\n",
    "    Nodo condicional que habilita un flujo human-in-the-loop.\n",
    "\n",
    "    Pregunta al usuario si desea realizar una b√∫squeda en internet para\n",
    "    complementar la respuesta. La decisi√≥n bifurca el grafo en dos ramas:\n",
    "    'yes' contin√∫a hacia la b√∫squeda web; 'no' salta directamente a la generaci√≥n\n",
    "    de respuesta con el contexto ya disponible.\n",
    "\n",
    "    La respuesta se incorpora al estado con la clave 'web_search', que se usa como\n",
    "    condici√≥n de control para el flujo en LangGraph.\n",
    "\n",
    "    Par√°metros:\n",
    "    - state (MemoryRAGState): Estado actual del sistema, que incluye la pregunta del usuario.\n",
    "\n",
    "    Retorna:\n",
    "    - dict: Estado extendido con la clave 'web_search' y el valor 'yes' o 'no',\n",
    "      dependiendo de la decisi√≥n del usuario.\n",
    "    \"\"\"\n",
    "    \n",
    "        question = state[\"question\"]\n",
    "        user_input = input(f\"üåê ¬øBuscar en internet para: '{question}'? (s/n): \")\n",
    "        decision = \"yes\" if user_input.lower() in [\"s\", \"si\", \"s√≠\", \"y\", \"yes\"] else \"no\"\n",
    "\n",
    "        # La clave debe coincidir con las ramas de decisi√≥n en el grafo\n",
    "        return {\"web_search\": decision, **state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a395cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document  \n",
    "from ddgs import DDGS\n",
    "\n",
    "def web_search_documents(state: MemoryRAGState) -> MemoryRAGState:\n",
    "        \n",
    "        \"\"\"\n",
    "    Nodo opcional del grafo que realiza una b√∫squeda en la web para enriquecer el contexto.\n",
    "\n",
    "    Utiliza el motor DuckDuckGo (`ddgs`) para obtener hasta tres resultados relevantes\n",
    "    en espa√±ol relacionados con la pregunta actual. Cada resultado se convierte en un \n",
    "    documento estructurado que se a√±ade a la lista `retrieved_docs` del estado.\n",
    "\n",
    "    Esto permite complementar la informaci√≥n recuperada de la base de datos vectorial\n",
    "    con contenido actualizado y proveniente de fuentes externas.\n",
    "\n",
    "    Si ocurre un error durante la b√∫squeda, el sistema lo registra en consola pero \n",
    "    contin√∫a su ejecuci√≥n sin interrumpir el flujo del grafo.\n",
    "\n",
    "    Par√°metros:\n",
    "    - state (MemoryRAGState): Estado actual del sistema, que contiene la pregunta del usuario.\n",
    "\n",
    "    Retorna:\n",
    "    - MemoryRAGState: El mismo estado, extendido con los nuevos documentos provenientes de la web \n",
    "      y metadatos sobre la cantidad de documentos a√±adidos.\n",
    "    \"\"\"\n",
    "\n",
    "        question = state[\"question\"]\n",
    "        web_docs = []\n",
    "\n",
    "        try:\n",
    "                # Configuraci√≥n de los par√°metros de navegaci√≥n\n",
    "                results = DDGS().text(\n",
    "                        query = question,\n",
    "                        region = \"es-es\",  \n",
    "                        safesearch = \"moderate\",\n",
    "                        max_results = 3,\n",
    "                        backend = \"auto\"\n",
    "                )\n",
    "\n",
    "                # Iteraci√≥n sobre los resultados para obtener el contenido relevante\n",
    "                for r in results:\n",
    "                        title = r.get(\"title\", \"Sin t√≠tulo\")\n",
    "                        snippet = r.get(\"body\", \"\")\n",
    "                        link = r.get(\"href\", \"#\")\n",
    "\n",
    "                        content = f\"\"\"üîó {title} {snippet} üëâ Enlace: {link}\"\"\"\n",
    "                        web_docs.append(Document(page_content = content, metadata = {\"source_file\": \"web_search\"}))\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "                print(f\"üåê Error en b√∫squeda web: {e}\")\n",
    "\n",
    "        # El contenido obtenido se concatena con el contexto recuperado de la base\n",
    "        # de datos vectorial\n",
    "        retrieved_docs = state.get(\"retrieved_docs\", []) + web_docs\n",
    "\n",
    "        # Se retorna el estado actualizado\n",
    "        return {\n",
    "                **state,\n",
    "                \"retrieved_docs\": retrieved_docs,\n",
    "                \"processing_metadata\": {\n",
    "                        **state.get(\"processing_metadata\", {}),\n",
    "                        \"web_docs_added\": len(web_docs)\n",
    "                }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b050155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memory_rag_graph_with_approval():\n",
    "    \n",
    "        \"\"\"\n",
    "    Construye y compila un grafo conversacional con memoria y b√∫squeda web opcional,\n",
    "    utilizando bifurcaci√≥n condicional seg√∫n la decisi√≥n del usuario.\n",
    "\n",
    "    El grafo sigue el siguiente flujo:\n",
    "    1. Recupera documentos desde la memoria.\n",
    "    2. Pregunta si se desea realizar una b√∫squeda en internet.\n",
    "    3. Si el usuario acepta, busca en la web y a√±ade los resultados al contexto.\n",
    "    4. Prepara el contexto combinando los documentos recuperados.\n",
    "    5. Genera una respuesta usando un modelo de lenguaje.\n",
    "    6. Persiste el estado tras cada paso, permitiendo retomar la conversaci√≥n m√°s adelante.\n",
    "    \"\"\"\n",
    "\n",
    "        # Inicializa el grafo con el estado que se usar√° entre nodos\n",
    "        workflow = StateGraph(MemoryRAGState)\n",
    "\n",
    "        # Define los nodos que forman el asistente\n",
    "        workflow.add_node(\"retrieve\", memory_retrieve_documents)     # Recuperar desde la memoria\n",
    "        workflow.add_node(\"ask_web_search\", should_search_web)       # Preguntar si se desea buscar en la web\n",
    "        workflow.add_node(\"web_search\", web_search_documents)        # Ejecutar b√∫squeda web (opcional)\n",
    "        workflow.add_node(\"prepare_context\", memory_prepare_context) # Preparar contexto combinado\n",
    "        workflow.add_node(\"generate\", memory_generate_response)      # Generar respuesta final\n",
    "\n",
    "        # Establece el punto de entrada del grafo\n",
    "        workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "        # Enlace directo del nodo de recuperaci√≥n al nodo de decisi√≥n sobre b√∫squeda web\n",
    "        workflow.add_edge(\"retrieve\", \"ask_web_search\")\n",
    "\n",
    "        # A√±ade bifurcaci√≥n condicional basada en la decisi√≥n del usuario\n",
    "        workflow.add_conditional_edges(\n",
    "                \"ask_web_search\",\n",
    "                lambda state: state[\"web_search\"],  # Accede al valor devuelto por el nodo de decisi√≥n\n",
    "                {\n",
    "                        \"yes\": \"web_search\",           # Si se aprueba, buscar en la web\n",
    "                        \"no\": \"prepare_context\"        # Si no, continuar directo a preparar el contexto\n",
    "                }\n",
    "        )\n",
    "\n",
    "        # Si se busc√≥ en la web, continuar con la preparaci√≥n del contexto\n",
    "        workflow.add_edge(\"web_search\", \"prepare_context\")\n",
    "\n",
    "        # Desde el contexto, proceder a generar la respuesta\n",
    "        workflow.add_edge(\"prepare_context\", \"generate\")\n",
    "\n",
    "        # Finalizar el flujo una vez generada la respuesta\n",
    "        workflow.add_edge(\"generate\", END)\n",
    "\n",
    "        # Compilar el grafo con persistencia activada\n",
    "        return workflow.compile(checkpointer = memory_checkpointer)\n",
    "\n",
    "# Crear y compilar la aplicaci√≥n\n",
    "memory_app = create_memory_rag_graph_with_approval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbce8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_graph():\n",
    "    \n",
    "        \"\"\"\n",
    "    Muestra el grafo en una celda interactiva. \n",
    "        Esta configurada espec√≠ficamente para notebook jupyter.\n",
    "    \"\"\"\n",
    "    \n",
    "        app = create_memory_rag_graph_with_approval()\n",
    "    \n",
    "        try:\n",
    "                display(Image(app.get_graph().draw_mermaid_png()))\n",
    "        except ImportError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2789fa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51683c1f",
   "metadata": {},
   "source": [
    "## Probando el asistente con memoria y aprobaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a56c124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_rag_with_langgraph_memory_human_in_loop(question: str, vector_store, session_id: str = None, k: int = 3) -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    Ejecuta el flujo de RAG con memoria y pasos de aprobaci√≥n (b√∫squeda web y env√≠o de correo).\n",
    "    \"\"\"\n",
    "\n",
    "    # Establece el vector store como variable global para evitar errores de serializaci√≥n\n",
    "    global global_vector_store\n",
    "    global_vector_store = vector_store\n",
    "\n",
    "    # Crea un id de sesi√≥n si no se proporciona\n",
    "    if not session_id:\n",
    "        import uuid\n",
    "        session_id = str(uuid.uuid4())[:8]\n",
    "\n",
    "    # Estado inicial para el grafo\n",
    "    initial_state = {\n",
    "        \"question\": question,\n",
    "        \"k\": k,\n",
    "        \"retrieved_docs\": [],\n",
    "        \"context_text\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"sources\": [],\n",
    "        \"conversation_history\": [],\n",
    "        \"session_id\": session_id,\n",
    "        \"user_context\": {},\n",
    "        \"processing_metadata\": {},\n",
    "    }\n",
    "\n",
    "    # Configuraci√≥n del grafo, se incluye el id de sesi√≥n para persistencia autom√°tica\n",
    "    config = {\"configurable\": {\"thread_id\": session_id}}\n",
    "\n",
    "    # Invocaci√≥n del grafo\n",
    "    final_state = memory_app.invoke(initial_state, config = config)\n",
    "\n",
    "        # Se retorna el resultdo de la ejecuci√≥n junto con informaci√≥n relevante \n",
    "    return {\n",
    "        \"question\": final_state[\"question\"],\n",
    "        \"answer\": final_state[\"answer\"],\n",
    "        \"sources\": [doc.metadata.get(\"source_file\", \"unknown\") for doc in final_state[\"retrieved_docs\"]],\n",
    "        \"session_id\": session_id,\n",
    "        \"used_memory\": final_state[\"processing_metadata\"].get(\"used_conversation_context\", False),\n",
    "        \"formatted_history\": final_state[\"processing_metadata\"].get(\"used_formatted_history\", False)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7d32681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_chat_with_approval(question, vector_store, session_id):\n",
    "    \n",
    "        \"\"\"\n",
    "    Ejecuta un turno conversacional usando memoria y b√∫squeda web si es aprobada.\n",
    "    \"\"\"\n",
    "      \n",
    "        # Llama al asistente con memoria y decisiones humanas (como b√∫squeda web)\n",
    "        result = ask_rag_with_langgraph_memory_human_in_loop(question, vector_store, session_id)\n",
    "\n",
    "        # Muestra pregunta y respuesta en consola\n",
    "        print(f\"üë§ Usuario: {question}\")\n",
    "        print(f\"ü§ñ {result['answer']}\")\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3eb72e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_test_with_approval():\n",
    "    \n",
    "    session = new_conversation()\n",
    "    print(f\"üÜï Nueva sesi√≥n creada: {session}\")\n",
    "    \n",
    "    print(\"\\nTest 1:\")\n",
    "    continue_chat_with_approval(\"Soy Ana, desarrolladora React\", vector_store, session)\n",
    "\n",
    "    print(\"\\nTest 2:\")\n",
    "    continue_chat_with_approval(\"¬øQu√© es React?\", vector_store, session)\n",
    "\n",
    "    print(\"\\nTest 3:\")\n",
    "    continue_chat_with_approval(\"¬øQu√© son las APIs REST?\", vector_store, session)\n",
    "\n",
    "    print(\"\\nTest 4:\")\n",
    "    continue_chat_with_approval(\"¬øQu√© pasa cuando tengo alta latencia en respuestas de API?\", vector_store, session)\n",
    "\n",
    "    print(\"\\nTest 5:\")\n",
    "    continue_chat_with_approval(\"¬øRecuerdas mi nombre?\", vector_store, session)\n",
    "\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "657087b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üÜï Nueva sesi√≥n creada: 0fa77899\n",
      "\n",
      "Test 1:\n",
      "üìù Ejecutando nodo: prepare_context\n",
      "üë§ Usuario: Soy Ana, desarrolladora React\n",
      "ü§ñ \n",
      "\n",
      "Test 2:\n",
      "üìù Ejecutando nodo: prepare_context\n",
      "üë§ Usuario: ¬øQu√© es React?\n",
      "ü§ñ Ana, seg√∫n el contexto de documentaci√≥n proporcionado, no tengo informaci√≥n sobre qu√© es React. El contexto se centra en est√°ndares de desarrollo de APIs, dise√±o RESTful y enlaces a foros de coches\n",
      "\n",
      "Test 3:\n",
      "üìù Ejecutando nodo: prepare_context\n",
      "üë§ Usuario: ¬øQu√© son las APIs REST?\n",
      "ü§ñ \n",
      "\n",
      "Test 4:\n",
      "üìù Ejecutando nodo: prepare_context\n",
      "üë§ Usuario: ¬øQu√© pasa cuando tengo alta latencia en respuestas de API?\n",
      "ü§ñ \n",
      "\n",
      "Test 5:\n",
      "üìù Ejecutando nodo: prepare_context\n",
      "üë§ Usuario: ¬øRecuerdas mi nombre?\n",
      "ü§ñ S√≠,\n"
     ]
    }
   ],
   "source": [
    "example_id = quick_test_with_approval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53acfc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç ESTADO DE SESI√ìN: 0fa77899\n",
      "==================================================\n",
      "üí¨ Mensajes en historial: 10\n",
      "üë§ Contexto de usuario: Soy Ana, desarrolladora React | ¬øRecuerdas mi nombre?\n",
      "üè∑Ô∏è √öltimo checkpoint: {'configurable': {'thread_id': '0fa77899', 'checkpoint_ns': '', 'checkpoint_id': '1f0c1023-c1c2-6c43-801f-3cdde7a7ba7f'}}\n",
      "\n",
      "üìñ √öltimos 3 mensajes:\n",
      "----------------------------------------\n",
      " 8. ü§ñ Asistente: \n",
      " 9. üë§ Usuario: ¬øRecuerdas mi nombre?\n",
      "10. ü§ñ Asistente: S√≠,\n",
      "\n",
      "üìä ESTAD√çSTICAS:\n",
      "   üó®Ô∏è  Total conversaciones: 10\n",
      "   üë§ Mensajes de usuario: 5\n",
      "   ü§ñ Respuestas del asistente: 5\n",
      "   üìã Info personal recopilada: 2 mensaje(s)\n",
      "   üìù √öltima info personal: ¬øRecuerdas mi nombre?...\n"
     ]
    }
   ],
   "source": [
    "inspect_langgraph_state(example_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
